---
title: "Why Does Sharpness-Aware Minimization Generalize Better Than SGD?"
collection: publications
permalink: /publication/paper-title-number-1
excerpt: ''
date: 2023-05-1
venue: 'Proc. of Advances in Neural Information Processing Systems'
paperurl: 'https://arxiv.org/pdf/2310.07269.pdf'
---

  - Characterized the conditions under which benign overfitting can occur in training two-layer convolutional ReLU networks with SGD.
  - Established a rigorous theoretical distinction between SAM and SGD, demonstrating that SAM strictly outperforms SGD in terms of generalization error.
  - Showed that SAM effectively mitigates noise learning in the early stages of training, enabling neural networks to learn features more efficiently.